<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://maxxxzdn.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://maxxxzdn.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-06-16T20:31:59+00:00</updated><id>https://maxxxzdn.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Erwin Transformer</title><link href="https://maxxxzdn.github.io/blog/erwin/" rel="alternate" type="text/html" title="Erwin Transformer" /><published>2025-06-13T00:00:00+00:00</published><updated>2025-06-13T00:00:00+00:00</updated><id>https://maxxxzdn.github.io/blog/erwin</id><content type="html" xml:base="https://maxxxzdn.github.io/blog/erwin/"><![CDATA[<div class="intro-gif">
  <video controls="" loop="" muted="" playsinline="" preload="metadata" poster="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/kino_poster.jpg">
    <source src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/kino.mp4" type="video/mp4" alt="Short video presentation of the paper." />
  </video>
</div>

<h2 id="introduction">Introduction</h2>
<p>Since attention<a href="#ref1">[1]</a> was introduced in 2017, a natural scientific instinct was to try to address its most obvious drawback - quadratic complexity. Many approaches emerged, specifically in the context of text and images. The main reason why numerous solutions exist for text and images is the regular structure of data. That is, if I tell you to look at position/column $j$, you know exactly where to look. Same with relative positions - there is no ambiguity when it comes to computing distance between two tokens in a sequence or two pixels in an image. Therefore, one can exploit this structure and derive a fixed attention pattern that is guaranteed to respect the geometry of the domain.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 15px; font-weight: bold;">
      Sparse attention exploits locality in regular data
    </p>
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/sparse_attention.svg" alt="Text input" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<h3 id="sparse-attention-on-irregular-data">Sparse Attention on Irregular Data</h3>
<p>Now, when it comes to irregular structures such as point clouds or unstructured meshes, the task becomes harder since data do not have an inherent ordering. This means that $i$-th and $j$-th elements are never guaranteed to be at the same distance from one another across different data points. As we cannot rely on regularity anymore, there is seemingly nothing to exploit - and the only solution would be to buckle up, make GPUs go brr and compute full attention.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 15px; font-weight: bold;">
      Sparse attention breaks locality in irregular data
    </p>
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/point_attention.svg" alt="Text input" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<h3 id="current-solutions-in-physical-modelling">Current Solutions in Physical Modelling</h3>
<p>This solution is, however, unrealistic in physical modelling - the field I am interested in - for two reasons: A) all the GPUs are taken by LLM folks, B) the number of points might reach well beyond 1 million. Furthermore, there is clearly a need for sparse attention for large scale physical systems, despite the limited number of solutions available. The most popular approach so far involves pooling to latent tokens/supernodes (Transolver<a href="#ref2">[2]</a>, UPT<a href="#ref3">[3]</a>), which makes the total cost sub-linear. Alternatively, one can project irregular data onto a regular domain, as one has full control over the latent node arrangement. This was done in Aurora<a href="#ref4">[4]</a> (and later in Appa<a href="#ref5">[5]</a>), which first uses Perceiver<a href="#ref6">[6]</a> for projection and then Swin Transformer<a href="#ref7">[7]</a> for handling attention over the latent regular grid, thus bringing the complexity down to log-linear. In between, there are solutions that have linear complexity and are based on turning a point cloud into a 1D sequence, e.g., via tree traversal (Octformer<a href="#ref8">[8]</a>) or space-filling curves (PointTransformer v3<a href="#ref9">[9]</a>).</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 15px; font-weight: bold;">
      Sub-quadratic attention for irregular data
    </p>
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start;">
      <figure style="width: 70%; margin: 0 auto;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/phys_attention.svg" alt="Text input" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<p>The advantage of the last approach is that it operates on native geometry and does not introduce an information bottleneck with pooling. Working with full fidelity is much closer in spirit to what people typically do in language modelling - the ultimate scaling frontier - with recent advancements in pushing the resolution even further with byte-level tokenization<a href="#ref10">[10]</a>. In general, I believe that this is what should be done at scale for as long as possible - let the data guide the model to decide which information to use and which not.</p>

<h2 id="tree-based-attention">Tree-based Attention</h2>
<p>Therefore, the goal for the project was to find a solution that would allow operation on the original full-scale representation of a system, capture long-range interactions, but avoid quadratic scaling of full attention. This problem traces back to 1980s many-body physics, where physicists needed to simulate particle systems with long-range interactions (electrostatic/gravitational potentials) at the scale of tens of millions of nodes. To avoid computing all-to-all pairwise interactions ($\approx 10^{12}$ for 1M nodes), one can resort to approximations (e.g. <a href="https://arborjs.org/docs/barnes-hut">Barnes-Hut algorithm</a>). Specifically, if there are many particles far away from a single node, one can treat those particles as a cluster and compute cluster-node interaction instead of pairwise interactions. This can be done at multiple scales, thus extending the range of captured interactions and their resolution. The computation is typically structured using hierarchical trees. For mode detils, check out the fantastic <a href="https://andyljones.com/posts/multipole-methods.html">blog by Andy Jones</a> on the Fast Multipole Method, who also implemented it in PyTorch - well worth a read.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start;">
      <figure style="width: 90%; margin: 0 auto;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/many_body_tree.svg" alt="Text input" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<p>This is the main idea behind Erwin - use hierarchical trees to impose regular structure on otherwise irregular data, then exploit this structure to organize attention and yield sub-quadratic complexity. Our first question was which tree to use. Traditionally, in many-body physics, octrees are used. These are built by recursively subdividing the cubic domain into sub-domains until each box contains at most a specified number of particles. This tree structure has even proved useful in the context of point cloud attention (see Octformer<a href="#ref8">[8]</a>, which traverses the tree to turn the underlying point cloud into a sequence suitable for sparse attention). However, we did not pursue octrees as the partitions they form might be highly unbalanced when particles cluster. This is particularly apparent when working with molecular dynamics, as shown in the figure below.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 70%; margin: 0px auto;">
    <p class="large-title" style="text-align: center; margin-bottom: 20px; font-size: 1.2em; font-weight: bold;">
      Data-driven domain partitioning with trees
    </p>
    <div style="width: 70%; display: flex; justify-content: center; margin: 0 auto;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/octree_vs_btree.svg" alt="Tree comparison" style="width: 100%; height: auto;" />
      </figure>
    </div>
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 0px;">
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-right: -20%">
        Quadtree
      </div>
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-right: 20%">
        Ball Tree
      </div>
    </div>
  </div>
</div>

<h3 id="ball-trees">Ball Trees</h3>
<p>On the contrary, ball trees, which we chose in the end, do not have this issue and are generally more robust to varying densities (although definitely not perfect, as we will see in the experiments later). Intuitively, the algorithm takes a bunch of points and covers them with a ball. Then, half of the points are covered with a smaller ball, and the remaining half with another ball such that no point belongs to both balls. The procedure is repeated until the smallest balls contain at most two points. You can now see why we chose to work with ball trees: they do not require covering the entire domain, only the points themselves, and at each tree level, the nodes are (often) associated with the same scale. This, in our opinion, makes ball trees an excellent candidate for handling irregular data, and we can now look at how they can be used to organize attention.</p>

<div class="intro-gif"> 
  <img src="https://raw.githubusercontent.com/maxxxzdn/erwin/main/misc/ball_tree_animation.gif" alt="Ball Tree Animation" style="width: 50%; margin-top: -40px; margin-bottom: -30px;" />
</div>

<p>Having built a tree, we can now compute attention within the ball tree partitions. Specifically, one can choose the level of the tree and compute attention (Ball Tree Attention, BTA) within the balls in parallel. This is essentially block attention, but now each block is guaranteed to contain tokens that are close to each other in Euclidean space. This spatial locality is natural for physical systems where interactions typically decay with distance - nearby particles have stronger forces between them, and local features often exhibit spatial coherence. Therefore, by grouping neighbouring points together, BTA aligns with the intuition that the most important interactions are captured within each attention block. This is the attention mechanism we use to capture fine local details in data.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 70%; margin: 0px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 0px; font-size: 1.2em; font-weight: bold;">
      Attention is computed within ball partitions
    </p>
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/bta.svg" alt="Ball Tree attention" style="width: 100%; height: auto;" />
      </figure>
    </div>
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 0px;">
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-left: -10%;">
        Full Attention: \( \mathcal{O}(N)^2 \)
      </div>
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-right: -30%;">
        Ball Tree Attention: \( \mathcal{O}(N) \)
      </div>
    </div>
  </div>
</div>

<h3 id="capturing-long-range-interactions">Capturing Long-range Interactions</h3>
<p>Now, obviously, block attention comes with a huge drawback - tokens cannot propagate information outside of their own blocks. This, of course, is unacceptable - we want to capture long-range interactions and hence have to introduce a mechanism for information to flow from one ball to another. We were inspired by Swin Transformer, which follows a very similar principle (the name Erwin is itself an homage to Swin Transformer, shifted windows $\rightarrow$ rotated windows). Specifically, while Swin uses sliding windows - essentially fixing the data and shifting the grid - we adapt the trick to irregular data. Specifically, we rotate the point cloud and build a second tree, exploiting the fact that ball tree construction is not rotation invariant, hence the new partitions will cover different groupings of points. By alternating between these two configurations in consecutive layers, we achieve information propagation beyond the original partitions.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 60%; margin: 20px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 0px; font-size: 1.2em; font-weight: bold;">
      Alternating between ball trees
    </p>
    <!-- Move captions to top -->
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 0px;">
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-left: -10%;">
        layer \( i \), tree 1
      </div>
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-right: -15%;">
        layer \( i+1 \), tree 2
      </div>
    </div>
    <!-- Figure comes after captions -->
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start; margin-bottom: 20px;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/tree_rot_alt.svg" alt="Alternating trees in Erwin" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<p>For large particle systems, however, this strategy still has limited receptive field and might be prone to problems arising in message passing - oversmoothing and oversquashing. Therefore, to capture very long-range interactions, we coarsen the tree and implement the model UNet-style. This is done trivially by simply pooling points in leaf balls to their center of mass, which halves the number of nodes. A nice property of ball trees is that the pooled nodes will still be contained in the original partitions all the way up to the root, hence we do not have to rebuild it.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 64%; margin: 20px 0;">
    <p class="large-title" style="text-align: center; margin-bottom: 0px; font-size: 1.2em; font-weight: bold;">
      Information propagation via coarsening
    </p>
    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; margin-bottom: 0px;">
    <div style="text-align: center; font-size: 1.0em; color: #666; margin-top: 5%; line-height: 1.5;">
      encoder<br />num nodes: 64
    </div>
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-top: 5%; line-height: 1.5;">
        bottleneck <br /> num nodes: 16
      </div>
      <div style="text-align: center; font-size: 1.0em; color: #666; margin-top: 5%; line-height: 1.5;">
        decoder <br /> num nodes: 64
      </div>
    </div>
    <div style="width: 100%; display: flex; gap: 3%; align-items: flex-start; margin-bottom: 20px;">
      <figure style="width: 100%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/coarsening.svg" alt="Coarsening for information propagation" style="width: 100%; height: auto;" />
      </figure>
    </div>
  </div>
</div>

<h3 id="memory-layout-of-erwin">Memory Layout of Erwin</h3>
<p>Perhaps the nicest property of ball trees is how they are stored in memory. The memory layout is contiguous and nested, meaning that smaller partitions within the same ball are stored next to each other. This makes all the operations related to tree handling extremely efficient: to build balls, you simply reshape the data tensor, and to pool, you simply call the mean operation. With the computational overhead minimized, it really boils down to the attention operation and tree building, both of which have to be optimized.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; max-width: 1000px; margin: 20px 0;">
    <!-- Main title -->
    <p class="large-title" style="text-align: center; margin-bottom: 15px; font-size: 1.2em; font-weight: bold; color: #333;">
      Ball tree is stored in memory contiguously
    </p> 
    <!-- Main figure with voxelization label -->
    <div style="width: 100%; display: flex; gap: 7%; align-items: flex-start;">
      <figure style="width: 38%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/mem_layout.svg" alt="Memory layout in Erwin" style="width: 100%; height: auto;" />
      </figure>
      <figure style="width: 62%; margin: 0;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/code.png" alt="Erwin pseudocode" style="width: 100%; height: auto;" />
      </figure>
    </div>
    <!-- Labels section -->
    <div style="position: relative; width: 100%;">
      <!-- Method names -->
      <div style="display: grid; grid-template-columns: 1fr 3fr; gap: 20px; margin-top: 15px; margin-bottom: -5px;">
        <div style="text-align: center; font-size: 1.0em; font-weight: bold; color: #333; margin-left: 10%;">
          Erwin Transformer
        </div>
        <div style="text-align: center; font-size: 1.0em; font-weight: bold; color: #333; margin-right: -15%;">
          Erwin pseudocode
        </div>
      </div>
      <!-- Descriptions -->
      <div style="display: grid; grid-template-columns: 1fr 3fr; gap: 20px; margin-bottom: 0px;">
        <div style="text-align: center; font-size: 1.0em; color: #666; margin-left: 10%;">
          ball tree
        </div>
        <div style="text-align: center; font-size: 1.0em; color: #666; margin-right: -15%;">
          partitioning is done via reshaping
        </div>
      </div>
    </div>
  </div>
</div>

<h3 id="efficient-ball-tree-implementation">Efficient Ball Tree implementation</h3>
<p>Luckily for us, there are plenty of extremely smart people working on the latter, so we focused on optimizing ball tree construction. Let’s be honest, if we claimed that our sparse attention is faster than full attention, but it takes seconds to organize the data, no one would be impressed. So, to impress people, we had to rewrite the implementation of scikit-learn which, while very efficient due to being written in C, does not allow handling batches, i.e., you have to build a tree for each element sequentially. To overcome this limitation, we (with great help from Claude, frankly speaking) implemented the construction in C++ and OpenMP such that the construction is parallelized over CPU cores and we can handle batches all in parallel, even with different point cloud sizes within the batch. This greatly reduces the runtime by a factor of 6 to 20 depending on the size of point clouds:</p>

<div class="l-body" style="display: flex; justify-content: space-between; gap: 40px; margin: 20px 0;">
  <div style="flex: 1;">
    <table style="width: 100%; border-collapse: collapse; font-size: 0.85em;">
      <thead>
        <tr style="border-bottom: 2px solid #333;">
          <th style="text-align: left; padding: 8px;">Implementation</th>
          <th style="text-align: center; padding: 8px;">1k nodes</th>
          <th style="text-align: center; padding: 8px;">2k nodes</th>
          <th style="text-align: center; padding: 8px;">4k nodes</th>
          <th style="text-align: center; padding: 8px;">8k nodes</th>
          <th style="text-align: center; padding: 8px;">16k nodes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="padding: 4px 8px;">sklearn + joblib</td>
          <td style="text-align: center; padding: 4px 8px;">15.6 ms</td>
          <td style="text-align: center; padding: 4px 8px;">16.3 ms</td>
          <td style="text-align: center; padding: 4px 8px;">21.2 ms</td>
          <td style="text-align: center; padding: 4px 8px;">24.1 ms</td>
          <td style="text-align: center; padding: 4px 8px;">44.0 ms</td>
        </tr>
        <tr style="border-top: 1px solid #ddd;">
          <td style="padding: 4px 8px;">Ours</td>
          <td style="text-align: center; padding: 4px 8px;">0.32 ms</td>
          <td style="text-align: center; padding: 4px 8px;">0.73 ms</td>
          <td style="text-align: center; padding: 4px 8px;">1.54 ms</td>
          <td style="text-align: center; padding: 4px 8px;">3.26 ms</td>
          <td style="text-align: center; padding: 4px 8px;">6.98 ms</td>
        </tr>
        <tr>
          <td style="padding: 4px 8px;">Speed-up</td>
          <td style="text-align: center; padding: 4px 8px;">48.8×</td>
          <td style="text-align: center; padding: 4px 8px;">22.3×</td>
          <td style="text-align: center; padding: 4px 8px;">13.8×</td>
          <td style="text-align: center; padding: 4px 8px;">7.4×</td>
          <td style="text-align: center; padding: 4px 8px;">6.3×</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<h2 id="experiments">Experiments</h2>

<p>Now, for the fun part - experiments. Before we present tables with bold numbers, let’s highlight some technical details about Erwin’s key properties: efficiency and global receptive field. Tree building consistently accounts for only a minor fraction of total computation time and can be precomputed before training for static geometries. To address the most frequently asked question: yes, you need to rebuild the tree at each time step when points move, but this is manageable because (A) our optimized implementation is fast, and (B) all layers share the same tree, amortizing the construction cost.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
      <figure style="width: 100%; margin: 0; text-align: center;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/scaling_and_rf.svg" alt="Molecular dynamics experiment" style="width: 70%; height: auto;" />
        <figcaption style="text-align: left;">
        <b>Left</b>: Computational cost of Erwin. We split the total
        runtime into building a ball tree and running a model. The input
        is a batch of 16 point clouds, each of size n. We fit a power law
        which indicates close to linear scaling. <b>Right</b>: Receptive field
        of MPNN vs Erwin, \(N\) = 800. A node is in the receptive field
        if changing its features affects the target node's output. MPNN
        consists of 6 layers, each node connected to 16 nearest neighbours.
        </figcaption>
      </figure>
  </div>
</div>

<h3 id="molecular-dynamics">Molecular Dynamics</h3>
<p>When we first started the project, I had large molecular systems in mind as a primary application. So this is exactly where I started: with the dataset from Xie et al., which contains dynamics of relatively large (coarse-grained, CG) polypeptides. The goal is to predict forces acting on each CG bead. As the main baseline we chose MPNN, which after some tuning turned out to be very tough to beat. In hindsight, this is not surprising as the dataset doesn’t have any charges, hence all the interactions are local. The experiment also confirmed to me just how good MPNNs are at picking up local interactions. In fact, we had to put a small MPNN before attention blocks to capture very fine local features - something that transformer alone wasn’t able to do (I wrote a <a href="https://twitter.com/maxxxzdn/status/1918303348177899992">recent thread</a> about this on Twitter). Nonetheless, Erwin does push the Pareto frontier, mainly because of how fast it is at the scale of this dataset.</p>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
      <figure style="width: 100%; margin: 0; text-align: center;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/md.svg" alt="Molecular dynamics experiment" style="width: 70%; height: auto;" />
        <figcaption style="text-align: left;">
        Test negative log-likelihood (NLL) of the predicted acceleration distribution for the molecular dynamics task (averaged
        over 3 runs). The baseline MPNN is taken from (Fu et al., 2022).
        The size of the markers reflects the number of parameters.
        </figcaption>
      </figure>
  </div>
</div>

<h3 id="standard-pde-benchmarks">Standard PDE Benchmarks</h3>
<p>Next, we benchmarked on a couple of PDE-related datasets - ShapeNet and datasets that people refer to as standard PDE benchmarks (specifically the ones that are not on regular grids, hence no Navier-Stokes or Darcy flow). Erwin shows incredibly strong performance on the majority of tasks but, interestingly, it fails on Airfoil - something that must be related to the structure of the mesh, where the density of points is decreasing moving away from the geometric center. We think this creates ball partitions with radically different densities, which complicates training. That being said, we never had issues with other non-uniform meshes. Another interesting finding was on the ShapeNet car dataset, where the strongest performance was achieved by models that did not involve any sort of pooling. That is, Erwin following a flat transformer structure (no coarse-graining of the tree) demonstrates state-of-the-art performance (for a sparse transformer, at least). This contrasts with other baselines that do involve pooling: GNO, UPT, Transolver. We attribute this to the sensitivity of the task to resolution - apparently one needs to work at full fidelity to capture all the necessary details.</p>

<div class="l-body" style="display: flex; justify-content: space-between; gap: 40px; margin: 20px 0;">
  <div style="flex: 1;">
    <h4>Standard PDE benchmarks</h4>
    <table style="width: 100%; border-collapse: collapse; font-size: 0.85em;">
      <thead>
        <tr style="border-bottom: 2px solid #333;">
          <th style="text-align: left; padding: 8px;">Model</th>
          <th style="text-align: center; padding: 8px;">Elasticity</th>
          <th style="text-align: center; padding: 8px;">Plasticity</th>
          <th style="text-align: center; padding: 8px;">Airfoil</th>
          <th style="text-align: center; padding: 8px;">Pipe</th>
        </tr>
      </thead>
      <tbody>
        <tr><td style="padding: 4px 8px;">LNO</td><td style="text-align: center; padding: 4px 8px;">0.69</td><td style="text-align: center; padding: 4px 8px;">0.29</td><td style="text-align: center; padding: 4px 8px;">0.53</td><td style="text-align: center; padding: 4px 8px;">0.31</td></tr>
        <tr><td style="padding: 4px 8px;">Galerkin</td><td style="text-align: center; padding: 4px 8px;">2.40</td><td style="text-align: center; padding: 4px 8px;">1.20</td><td style="text-align: center; padding: 4px 8px;">1.18</td><td style="text-align: center; padding: 4px 8px;">0.98</td></tr>
        <tr><td style="padding: 4px 8px;">HT-Net</td><td style="text-align: center; padding: 4px 8px;">/</td><td style="text-align: center; padding: 4px 8px;">3.33</td><td style="text-align: center; padding: 4px 8px;">0.65</td><td style="text-align: center; padding: 4px 8px;">0.59</td></tr>
        <tr><td style="padding: 4px 8px;">OFormer</td><td style="text-align: center; padding: 4px 8px;">1.83</td><td style="text-align: center; padding: 4px 8px;">0.17</td><td style="text-align: center; padding: 4px 8px;">1.83</td><td style="text-align: center; padding: 4px 8px;">1.68</td></tr>
        <tr><td style="padding: 4px 8px;">GNOT</td><td style="text-align: center; padding: 4px 8px;">0.86</td><td style="text-align: center; padding: 4px 8px;">3.36</td><td style="text-align: center; padding: 4px 8px;">0.76</td><td style="text-align: center; padding: 4px 8px;">0.47</td></tr>
        <tr><td style="padding: 4px 8px;">FactFormer</td><td style="text-align: center; padding: 4px 8px;">/</td><td style="text-align: center; padding: 4px 8px;">3.12</td><td style="text-align: center; padding: 4px 8px;">0.71</td><td style="text-align: center; padding: 4px 8px;">0.60</td></tr>
        <tr><td style="padding: 4px 8px;">ONO</td><td style="text-align: center; padding: 4px 8px;">1.18</td><td style="text-align: center; padding: 4px 8px;">0.48</td><td style="text-align: center; padding: 4px 8px;">0.61</td><td style="text-align: center; padding: 4px 8px;">0.52</td></tr>
        <tr><td style="padding: 4px 8px;">Transolver++</td><td style="text-align: center; padding: 4px 8px;">0.52</td><td style="text-align: center; padding: 4px 8px;">0.11</td><td style="text-align: center; padding: 4px 8px;"><b>0.48</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.27</b></td></tr>
        <tr style="border-top: 1px solid #ddd;"><td style="padding: 4px 8px;"><b>Erwin (Ours)</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.34</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.10</b></td><td style="text-align: center; padding: 4px 8px;">2.57</td><td style="text-align: center; padding: 4px 8px;">0.61</td></tr>
      </tbody>
    </table>
  </div>
  <div style="flex: 1;">
    <h4>ShapeNet-Car</h4>
    <table style="width: 100%; border-collapse: collapse; font-size: 0.85em;">
      <thead>
        <tr style="border-bottom: 2px solid #333;">
          <th style="text-align: left; padding: 8px;">Model</th>
          <th style="text-align: center; padding: 8px;">MSE</th>
        </tr>
      </thead>
      <tbody>
        <tr><td style="padding: 4px 8px;">PointNet</td><td style="text-align: center; padding: 4px 8px;">43.36</td></tr>
        <tr><td style="padding: 4px 8px;">GINO</td><td style="text-align: center; padding: 4px 8px;">35.24</td></tr>
        <tr><td style="padding: 4px 8px;">UPT</td><td style="text-align: center; padding: 4px 8px;">31.66</td></tr>
        <tr><td style="padding: 4px 8px;">Transolver</td><td style="text-align: center; padding: 4px 8px;">19.88</td></tr>
        <tr><td style="padding: 4px 8px;">GP-UPT</td><td style="text-align: center; padding: 4px 8px;">17.02</td></tr>
        <tr><td style="padding: 4px 8px;">PTV3-S</td><td style="text-align: center; padding: 4px 8px;">19.09</td></tr>
        <tr><td style="padding: 4px 8px;">PTV3-M</td><td style="text-align: center; padding: 4px 8px;">17.42</td></tr>
        <tr style="border-top: 1px solid #ddd;"><td style="padding: 4px 8px;"><b>Erwin-S (Ours)</b></td><td style="text-align: center; padding: 4px 8px;"><b>15.85</b></td></tr>
        <tr><td style="padding: 4px 8px;"><b>Erwin-M (Ours)</b></td><td style="text-align: center; padding: 4px 8px;"><b>15.43</b></td></tr>
      </tbody>
    </table>
  </div>
</div>

<h3 id="turbulent-fluid-dynamics">Turbulent Fluid Dynamics</h3>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
      <figure style="width: 100%; margin: 0; text-align: center;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/eagle.svg" alt="Molecular dynamics experiment" style="width: 70%; height: auto;" />
        <figcaption style="text-align: left;">
        The norm of the velocity field at different steps of the rollout trajectories.
        </figcaption>
      </figure>
  </div>
</div>

<p>The final experiment is the EAGLE dataset, which is a large-scale collection of turbulent fluid dynamics trajectories on irregular meshes of different shapes, approximately 3.5k nodes on average. There is nothing really interesting to say except for just how well a good model can learn and for how long it can unroll seemingly complex trajectories autoregressively. It was very impressive to see, and even better to see Erwin do it the best :).</p>

<div class="l-body" style="margin: 20px 0;">
  <h4>RMSE on velocity V and pressure P fields across different prediction horizons</h4>
  <table style="width: 100%; border-collapse: collapse; font-size: 0.9em;">
    <thead>
        <th rowspan="2" style="text-align: left; padding: 8px; border-right: 1px solid #ddd;">Model</th>
        <th colspan="2" style="text-align: center; padding: 8px; border-right: 1px solid #ddd;">\(+\Delta t \)</th>
        <th colspan="2" style="text-align: center; padding: 8px; border-right: 1px solid #ddd;">\(+50\Delta t \)</th>
        <th rowspan="2" style="text-align: center; padding: 8px; border-right: 1px solid #ddd;">Time<br />(ms)</th>
        <th rowspan="2" style="text-align: center; padding: 8px;">Mem.<br />(GB)</th>
      <tr style="border-bottom: 2px solid #333;">
        <th style="text-align: center; padding: 8px;">V</th>
        <th style="text-align: center; padding: 8px; border-right: 1px solid #ddd;">P</th>
        <th style="text-align: center; padding: 8px;">V</th>
        <th style="text-align: center; padding: 8px; border-right: 1px solid #ddd;">P</th>
      </tr>
    </thead>
    <tbody>
      <tr><td style="padding: 4px 8px;">MGN</td><td style="text-align: center; padding: 4px 8px;">0.081</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">0.43</td><td style="text-align: center; padding: 4px 8px;">0.592</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">2.25</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">40</td><td style="text-align: center; padding: 4px 8px;">0.7</td></tr>
      <tr><td style="padding: 4px 8px;">GAT</td><td style="text-align: center; padding: 4px 8px;">0.170</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">64.6</td><td style="text-align: center; padding: 4px 8px;">0.855</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">163</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">44</td><td style="text-align: center; padding: 4px 8px;">0.5</td></tr>
      <tr><td style="padding: 4px 8px;">DRN</td><td style="text-align: center; padding: 4px 8px;">0.251</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">1.45</td><td style="text-align: center; padding: 4px 8px;">0.537</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">2.46</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">42</td><td style="text-align: center; padding: 4px 8px;"><b>0.2</b></td></tr>
      <tr><td style="padding: 4px 8px;">EAGLE</td><td style="text-align: center; padding: 4px 8px;">0.053</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">0.46</td><td style="text-align: center; padding: 4px 8px;">0.349</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">1.44</td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;">30</td><td style="text-align: center; padding: 4px 8px;">1.5</td></tr>
      <tr style="border-top: 1px solid #ddd;"><td style="padding: 4px 8px;"><b>Erwin<br />(Ours)</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.044<br />±0.001</b></td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;"><b>0.31<br />±0.01</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.281<br />±0.001</b></td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;"><b>1.15<br />±0.06</b></td><td style="text-align: center; padding: 4px 8px; border-right: 1px solid #ddd;"><b>11</b></td><td style="text-align: center; padding: 4px 8px;"><b>0.2</b></td></tr>
    </tbody>
  </table>
</div>

<h2 id="future-work">Future Work</h2>

<p>Regarding future work, there are definitely exciting directions to explore given how simple and even trivial the current attention pattern is, and how much more can be done by exploiting the ball tree structure to make the model more expressive. Below are two projects that I am working on with my students.</p>

<h3 id="erwin-meets-native-sparse-attention">Erwin Meets Native Sparse Attention</h3>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
      <figure style="width: 100%; margin: 0; text-align: center;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/bsa.svg" alt="Molecular dynamics experiment" style="width: 60%; height: auto;" />
        <figcaption style="text-align: left;">
        Ball Sparse Attention (BSA). A ball tree imposes spatial locality, then three sparse-attention branches—grouping (block clustering), compression (MLP-based token pooling), and selection (top-k block retrieval) operate alongside fine-grained ball attention. A learnable gate fuses their outputs into the final attention.
        </figcaption>
      </figure>
  </div>
</div>

<p>To improve the processing of long-range informaition while keeping the cost sub-quadratic, we combine the ball tree with Native Sparse Attention (NSA) from DeepSeek, which align naturally with each other. Specifically, NSA allows Erwin to learn for each leaf node to which partitions to attend in the tree in a data-driven manner, thus extending interactions beyond fixed balls. We already have some promising results that I will be presenting at ICML 2025 Workshop on Long-Context Foundation Models, see <a href="https://openreview.net/forum?id=VbPd9V6Wm0">the paper</a> and <a href="https://github.com/britacatalin/bsa">the code</a>.</p>

<h3 id="erwin-for-industrial-scale-applications">Erwin for Industrial Scale Applications</h3>

<div class="l-body" style="display: flex; justify-content: center;">
  <div style="width: 100%; margin: 20px 0;">
      <figure style="width: 100%; margin: 0; text-align: center;">
        <img src="https://raw.githubusercontent.com/maxxxzdn/_erwin-visuals/main/haet.jpg" alt="Molecular dynamics experiment" style="width: 60%; height: auto;" />
        <figcaption style="text-align: left;">
        To process industrial scale systems, we use the slicing approach of Transolver and then process them with Ball Tree attention.
        </figcaption>
      </figure>
  </div>
</div>

<p>Despite its linear complexity, applications involving simulations with tens of millions of particles currently remain beyond Erwin’s reach. To address this, we are exploring a hybrid approach: combining Transolver with Erwin by applying ball tree attention over latent tokens. The key insight is that by using Erwin to process these supernodes, we can afford larger bottleneck sizes (more supernodes) while maintaining efficiency. This allows us to significantly reduce the compression ratio compared to Transolver, preserving more information while keeping computational costs manageable. <a href="https://github.com/pedrocurvo/HAET">Initial results</a> are promising - stay tuned!</p>

<h2 id="references">References</h2>
<div id="ref1">[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: Attention is All you Need. NIPS 2017</div>
<div id="ref2">[2] Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long: Transolver: A Fast Transformer Solver for PDEs on General Geometries. ICML 2024</div>
<div id="ref3">[3] Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter:
Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators. NeurIPS 2024</div>
<div id="ref4">[4] Cristian Bodnar, Wessel P. Bruinsma, Ana Lucic, Megan Stanley, Anna Allen, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan A. Weyn, Haiyu Dong, Jayesh K. Gupta, Kit Thambiratnam, Alexander T. Archibald, Chun-Chieh Wu, Elizabeth Heider, Max Welling, Richard E. Turner, Paris Perdikaris:
A foundation model for the Earth system. Nature 641(8065): 1180-1187 (2025)</div>
<div id="ref5">[5] Gérôme Andry, François Rozet, Sacha Lewin, Omer Rochman Sharabi, Victor Mangeleer, Matthias Pirlet, Elise Faulx, Marilaure Grégoire, Gilles Louppe:
Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation. CoRR abs/2504.18720 (2025)</div>
<div id="ref6">[6] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, João Carreira:
Perceiver: General Perception with Iterative Attention. ICML 2021</div>
<div id="ref7">[7] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo:
Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021</div>
<div id="ref8">[8] Peng-Shuai Wang: OctFormer: Octree-based Transformers for 3D Point Clouds. SIGGRAPH 2023</div>
<div id="ref9">[9] 	Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao:
Point Transformer V3: Simpler, Faster, Stronger. CVPR 2024</div>
<div id="ref10">[10] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodríguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srinivasan Iyer:
Byte Latent Transformer: Patches Scale Better Than Tokens. CoRR abs/2412.09871 (2024)</div>]]></content><author><name></name></author><category term="attention" /><category term="trees" /><summary type="html"><![CDATA[We can organize irregular data (point clouds, meshes) using ball trees to emplot sub-quadratic (sparse) attention. Fast and expressive!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://maxxxzdn.github.io/erwin.jpg" /><media:content medium="image" url="https://maxxxzdn.github.io/erwin.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Clifford-Steerable CNNs</title><link href="https://maxxxzdn.github.io/blog/cscnns/" rel="alternate" type="text/html" title="Clifford-Steerable CNNs" /><published>2024-08-06T23:59:59+00:00</published><updated>2024-08-06T23:59:59+00:00</updated><id>https://maxxxzdn.github.io/blog/cscnns</id><content type="html" xml:base="https://maxxxzdn.github.io/blog/cscnns/"><![CDATA[<p>Redirecting…</p>

<script>
    window.location.href = "/blog/cscnns.html";
</script>]]></content><author><name></name></author><category term="equivariance" /><category term="cliffordalgerba" /><summary type="html"><![CDATA[Using Clifford algebra allows us to generalize E(n)-equivariant CNNs to E(p,q), which now includes isometries of spacetime!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://maxxxzdn.github.io/clifford_steerable.png" /><media:content medium="image" url="https://maxxxzdn.github.io/clifford_steerable.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Implicit Steerable Kernels</title><link href="https://maxxxzdn.github.io/blog/implicit_kernels/" rel="alternate" type="text/html" title="Implicit Steerable Kernels" /><published>2023-10-09T23:59:59+00:00</published><updated>2023-10-09T23:59:59+00:00</updated><id>https://maxxxzdn.github.io/blog/implicit_kernels</id><content type="html" xml:base="https://maxxxzdn.github.io/blog/implicit_kernels/"><![CDATA[<p>Redirecting…</p>

<script>
    window.location.href = "/blog/implicit_kernels.html";
</script>]]></content><author><name></name></author><category term="equivariance" /><category term="steerableCNNs" /><summary type="html"><![CDATA[Using implicit parameterization of G-steerable kernels to simplify designing steerable CNNs. Let me show you how.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://maxxxzdn.github.io/inf_kernels.png" /><media:content medium="image" url="https://maxxxzdn.github.io/inf_kernels.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>