---
layout: post
date: 2025-12-18 07:59:00-0400
inline: true
related_posts: false
---

[ðŸš¨ New paper] We depeloped [MSPT](https://arxiv.org/abs/2512.01738) - parallelized multi-scale attention method based on hierarchical partitioning of data. It is incredibly fast and achieves SOTA performance on multiple PDE tasks.