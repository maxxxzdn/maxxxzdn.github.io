---
---

@article{pedro2025mspt,
      abbr={preprint},
      journal={preprint},
      title={MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention}, 
      author={Pedro Curvo and Jan-Willem {van de Meent} and Maksim Zhdanov},
      abstract={A key scalability challenge in neural solvers for industrial-scale physics simulations is efficiently capturing both fine-grained local interactions and long-range global dependencies across millions of spatial elements. We introduce the Multi-Scale Patch Transformer (MSPT), an architecture that combines local point attention within patches with global attention to coarse patch-level representations. To partition the input domain into spatially-coherent patches, we employ ball trees, which handle irregular geometries efficiently. This dual-scale design enables MSPT to scale to millions of points on a single GPU. We validate our method on standard PDE benchmarks (elasticity, plasticity, fluid dynamics, porous flow) and large-scale aerodynamic datasets (ShapeNet-Car, Ahmed-ML), achieving state-of-the-art accuracy with substantially lower memory footprint and computational cost.},
      url={https://arxiv.org/abs/2512.01738}, 
      pdf={https://arxiv.org/pdf/2512.01738},
      arxiv = {2512.01738},
      preview = {mspt.png},
      year_ = 2025,
      selected = {false},
}


@article{winfried2025adaquant,
      abbr={TMLR},
      journal={TMLR},
      title={Adaptive Mesh Quantization for Neural PDE Solvers}, 
      author={Winfried {van den Dool} and Maksim Zhdanov and Yuki Asano and Max Welling},
      abstract={Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier–Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50\% improvements in performance at the same cost.},
      url={https://openreview.net/forum?id=NN17y897WG}, 
      pdf={https://openreview.net/pdf?id=NN17y897WG},
      arxiv = {2511.18474},
      code = {https://github.com/Winfriedvdd/AMQ},
      preview = {adaquant.png},
      year_ = 2025,
      selected = {false},
}


@article{balint2025csccns,
      abbr={AI4Science @ NeurIPS},
      journal={AI4Science Workshop @ NeurIPS 2025},
      title={Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling}, 
      author={Bálint László Szarvas and Maksim Zhdanov},
      abstract={Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows incorporating equivariance to arbitrary pseudo-Euclidean groups, including isometries of Euclidean space and Minkowski spacetime. In this work, we demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the model expressivity. To address this issue, we propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. We derive the equivariance constraint for these input-dependent kernels and show how it can be solved efficiently via implicit parameterization. We empirically demonstrate an improved expressivity of the resulting framework on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where our method consistently outperforms baseline methods.},
      url={https://arxiv.org/abs/2510.14007}, 
      pdf={https://arxiv.org/pdf/2510.14007.pdf},
      arxiv = {2510.14007},
      code = {https://github.com/maxxxzdn/clifford-group-equivariant-cnns},
      preview = {ccscnn.png},
      year_ = 2025,
      selected = {false},
}


@article{brita2025bsaballsparseattention,
      journal={LCFM @ ICML 2025},
      abbr = {LCFM @ ICML},
      title={BSA: Ball Sparse Attention for Large-scale Geometries}, 
      author={Catalin E. Brita and Hieu Nguyen and Lohithsai Yadala Chanchu and Domonkos Nagy and Maksim Zhdanov},
      abstract={Self-attention scales quadratically with input size, limiting its use for large-scale physical systems. Although sparse attention mechanisms provide a viable alternative, they are primarily designed for regular structures such as text or images, making them inapplicable for irregular geometries. In this work, we present Ball Sparse Attention (BSA), which adapts Native Sparse Attention (NSA) (Yuan et al., 2025) to unordered point sets by imposing regularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et al., 2025). We modify NSA's components to work with ball-based neighborhoods, yielding a global receptive field at sub-quadratic cost. On an airflow pressure prediction task, we achieve accuracy comparable to Full Attention while significantly reducing the theoretical computational complexity.},
      url={https://arxiv.org/abs/2506.12541}, 
      pdf={https://arxiv.org/pdf/2506.12541.pdf},
      arxiv = {2506.12541},
      code = {https://github.com/britacatalin/bsa},
      preview = {bsa.png},
      year_ = 2025,
      selected = {false},
}

@article{zhdanov2025erwintreebasedhierarchicaltransformer,
      journal={ICML 2025},
      abbr = {ICML},
      title={Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems}, 
      author={Maksim Zhdanov and Max Welling and Jan-Willem {van de Meent}},
      abstract={Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.},
      selected = {true},
      url={https://arxiv.org/abs/2502.17019}, 
      pdf={https://arxiv.org/pdf/2502.17019.pdf},
      code = {https://github.com/maxxxzdn/erwin},
      arxiv = {2502.17019},
      preview = {erwin.jpg},
      blog = {/blog/erwin/},
      year_ = 2025,
}

@article{zhdanov2025adsgnn,
      abbr={preprint},
      journal={preprint},
      title={AdS-{GNN} - a Conformally Equivariant Graph Neural Network},
      author={Maksim Zhdanov and Nabil Iqbal and Erik J Bekkers and Patrick Forr{\'e}},
      abstract={Conformal symmetries, i.e.\ coordinate transformations that preserve angles, play a key role in many fields, including physics, mathematics, computer vision and (geometric) machine learning. Here we build a neural network that is equivariant under general conformal transformations. To achieve this, we lift data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to exploit a known correspondence between conformal transformations of flat space and isometric transformations on the AdS space. We then build upon the fact that such isometric transformations have been extensively studied on general geometries in the geometric deep learning literature. We employ message-passing layers conditioned on the proper distance, yielding a computationally efficient framework. We validate our model on tasks from computer vision and statistical physics, demonstrating strong performance, improved generalization capacities, and the ability to extract conformal data such as scaling dimensions from the trained network.},
      selected = {true},
      arxiv={2505.12880}, 
      url={https://arxiv.org/abs/2505.12880}, 
      pdf={https://arxiv.org/pdf/2505.12880},
      code = {https://github.com/maxxxzdn/adsgnn},
      preview = {adsgnn.png},
      year_ = 2025,
}


@article{2402.14730,
  abbr = {ICML},
  journal={ICML 2024},
  arxiv = {2402.14730},
  abstract = {We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of \(E(p, q)\)-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces. They cover, for instance, \(E(3)\)-equivariance and Poincaré-equivariance on Minkowski spacetime. Our approach is based on an implicit parametrization of \(O(p,q)\)-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.},
  url = {https://arxiv.org/abs/2402.14730},
  pdf = {https://arxiv.org/pdf/2402.14730.pdf},
  author = {Zhdanov, Maksim and Ruhe, David and Weiler, Maurice and Lucic, Ana and Brandstetter, Johannes and Forré, Patrick},
  title = {Clifford-Steerable Convolutional Neural Networks},
  selected = {true},
  code = {https://github.com/maxxxzdn/clifford-group-equivariant-cnns},
  preview = {clifford_steerable.png},
  blog = {/blog/cscnns.html},
  year_ = 2024,
}

@article{https://doi.org/10.48550/arxiv.2212.06096,
  doi = {10.48550/ARXIV.2212.06096},
  abbr = {NeurIPS},
  abstract = {Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group G, such as reflections and rotations. They rely on standard convolutions with G-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group G, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize G-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group G for which a G-equivariant MLP can be built. We apply our method to point cloud (ModelNet-40) and molecular data (QM9) and demonstrate a significant improvement in performance compared to standard Steerable CNNs.},
  url = {https://arxiv.org/abs/2212.06096},
  journal={NeurIPS 2023},
  arxiv = {2212.06096},
  pdf = {https://arxiv.org/pdf/2212.06096.pdf},
  url = {https://arxiv.org/abs/2212.06096},
  author = {Zhdanov, Maksim and Hoffmann, Nico and Cesa, Gabriele},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Implicit Neural Convolutional Kernels for Steerable CNNs},
  publisher = {arXiv},
  copyright = {Creative Commons Attribution 4.0 International},
  selected = {false},
  preview = {imp_kernels.png},
  code = {https://github.com/maxxxzdn/implicit-steerable-kernels},
  blog = {/blog/implicit_kernels.html},
  year_ = 2023,
}

@article{https://doi.org/10.48550/arxiv.2206.01930,
  doi = {10.48550/ARXIV.2206.01930},
  abbr = {ICPR},
  journal={ICPR 2022},
  abstract = {Functional connectivity plays an essential role in modern neuroscience. The modality sheds light on the brain's functional and structural aspects, including mechanisms behind multiple pathologies. One such pathology is schizophrenia which is often followed by auditory verbal hallucinations. The latter is commonly studied by observing functional connectivity during speech processing. In this work, we have made a step toward an in-depth examination of functional connectivity during a dichotic listening task via deep learning for three groups of people: schizophrenia patients with and without auditory verbal hallucinations and healthy controls. We propose a graph neural network-based framework within which we represent EEG data as signals in the graph domain. The framework allows one to 1) predict a brain mental disorder based on EEG recording, 2) differentiate the listening state from the resting state for each group and 3) recognize characteristic task-depending connectivity. Experimental results show that the proposed model can differentiate between the above groups with state-of-the-art performance. Besides, it provides a researcher with meaningful information regarding each group's functional connectivity, which we validated on the current domain knowledge.},
  url = {https://arxiv.org/abs/2206.01930},
  arxiv = {2206.01930},
  pdf = {https://arxiv.org/pdf/2206.01930.pdf},
  code = {https://github.com/maxxxzdn/EEGCN},
  author = {Zhdanov, Maksim and Steinmann, Saskia and Hoffmann, Nico},
  keywords = {Neurons and Cognition (q-bio.NC), Machine Learning (cs.LG), FOS: Biological sciences, FOS: Biological sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Investigating Brain Connectivity with Graph Neural Networks and GNNExplainer},
  publisher = {arXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected = {false},
  preview = {gnns.png},
  year_ = 2022,
}

@article{https://doi.org/10.48550/arXiv.2210.01543,
  doi = {10.48550/ARXIV.2210.01543},
  abbr = {ML4PS@NeurIPS},
  abstract = {Grazing-Incidence Small-Angle X-ray Scattering (GISAXS) is a modern imaging technique used in material research to study nanoscale materials. Reconstruction of the parameters of an imaged object imposes an ill-posed inverse problem that is further complicated when only an in-plane GISAXS signal is available. Traditionally used inference algorithms such as Approximate Bayesian Computation (ABC) rely on computationally expensive scattering simulation software, rendering analysis highly time-consuming. We propose a simulation-based framework that combines variational auto-encoders and normalizing flows to estimate the posterior distribution of object parameters given its GISAXS data. We apply the inference pipeline to experimental data and demonstrate that our method reduces the inference cost by orders of magnitude while producing consistent results with ABC.},
  url = {https://arxiv.org/abs/2210.01543},
  journal={ML4PS@NeurIPS 2022},
  arxiv = {2210.01543},
  pdf = {https://arxiv.org/pdf/2210.01543.pdf},
  code = {https://github.com/maxxxzdn/gisaxs-reconstruction},
  author = {Zhdanov, Maksim and Randolph, Lisa and Kluge, Thomas and Nakatsutsumi, Motoaki and Gutt, Christian and Ganeva, Marina and Hoffmann, Nico},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Amortized Bayesian Inference of GISAXS Data with Normalizing Flows},
  publisher = {arXiv},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  selected = {false},
  preview = {gisaxs.png},
  year_ = 2022,
}

@article{https://doi.org/10.48550/arxiv.2206.01939,
  doi = {10.48550/ARXIV.2206.01939},
  abbr = {DGM4MICCAI},
  abstract={Electroencephalography produces high-dimensional, stochastic data from which it might be challenging to extract high-level knowledge about the phenomena of interest. We address this challenge by applying the framework of variational auto-encoders to 1) classify multiple pathologies and 2) recover the neurological mechanisms of those pathologies in a data-driven manner. Our framework learns generative factors of data related to pathologies. We provide an algorithm to decode those factors further and discover how different pathologies affect observed data. We illustrate the applicability of the proposed approach to identifying schizophrenia, either followed or not by auditory verbal hallucinations. We further demonstrate the ability of the framework to learn disease-related mechanisms consistent with current domain knowledge. We also compare the proposed framework with several benchmark approaches and indicate its classification performance and interpretability advantages.},
  url = {https://arxiv.org/abs/2206.01939},
  journal={DGM4MICCAI 2022},
  arxiv = {2206.01939},
  pdf = {https://arxiv.org/pdf/2206.01939.pdf},
  code = {https://github.com/maxxxzdn/eegVAE},
  author = {Zhdanov, Maksim and Steinmann, Saskia and Hoffmann, Nico},
  keywords = {Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {Learning Generative Factors of EEG Data with Variational auto-encoders},
  publisher = {arXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected = {false},
  preview = {eegvae.png},
  year_ = 2022,
}