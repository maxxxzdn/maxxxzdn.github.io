<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <meta name="description" content="CSCNNs blog">
  <meta property="og:title" content="Clifford-Steerable Convolutional Neural Networks"/>
  <meta property="og:description" content="Blog post"/>
  <meta property="og:url" content="maxxxzdn.github.io"/>
  <meta property="og:image" content="../assets/CSCNNs_/static/images/cscnns_main.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="725"/>

  <meta name="twitter:title" content="maxxxzdn">
  <meta name="twitter:description" content="blog">
  <meta name="keywords" content="deep learning, equivariance, symmetry, spacetime">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>

  <title>Clifford-Steerable Convolutional Neural Networks</title>
  <link rel="icon" type="image/x-icon" href="../assets/CSCNNs_/static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="../assets/CSCNNs_/static/css/bulma.min.css">
  <link rel="stylesheet" href="../assets/CSCNNs_/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../assets/CSCNNs_/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../assets/CSCNNs_/static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../assets/CSCNNs_/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../assets/CSCNNs_/static/js/fontawesome.all.min.js"></script>
  <script src="../assets/CSCNNs_/static/js/bulma-carousel.min.js"></script>
  <script src="../assets/CSCNNs_/static/js/bulma-slider.min.js"></script>
  <script src="../assets/CSCNNs_/static/js/index.js"></script>

  <style>
    .container.is-wider-desktop {
      max-width: 1200px !important;
      width: 90% !important;
    }

    .img-container2 {
        display: flex;
        align-items: center;
        justify-content: space-between;
        width: 90%;
        margin-left: 15px;
    }

    .hero.is-custom-light {
        background-color: #FFF5E1
    }

    .footer {
        background-color: #f5f5f5;
        padding: 20px 0;
    }

    .img-container2 img {
        height: 220px;
    }

    .img-container2 img:first-child {
        height: 220px;
        margin-right: 50px;
    }

    .img-container {
        display: flex;
        align-items: center;
        justify-content: space-between;
        width: 80%;
        margin-left: -20px;
    }

    .img-container img {
        height: 170px;
        max-height: 200px;
    }

    .img-container img:first-child {
        height: 170px;
        margin-right: 50px;
    }

    .teaser-image {
        width: 100%;
        max-width: 600px;
        padding: 20px;
        background-color: white;
        display: block;
        margin: 0 auto;
    }

    .theorem-box {
        border: 2px solid black;
        padding: 10px;
        margin: 20px 0;
        background-color: #f5f5f5;
    }

    .theorem-box strong {
        color: darkred;
    }

    .content-columns {
        display: flex;
        flex-wrap: wrap;
    }

    .content-column {
        flex: 1;
        padding: 20px;
    }

    .text-column {
        width: 60%;
        padding-right: 10px;
    }

    .image-column {
        width: 40%;
        padding-left: 10px;
        padding-top: 30px;
    }

    .image-column img {
        margin-bottom: 20px;
        width: 100%;
        height: auto;
    }

  </style>
</head>
<body>
  <section class="hero is-custom-light">
    <div class="hero-body">
      <div class="container is-wider-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Clifford-Steerable Convolutional Neural Networks</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://maxxxzdn.github.io/" target="_blank">Maksim Zhdanov</a><sup>1</sup>,
                <a href="https://davidruhe.github.io/" target="_blank">David Ruhe</a><sup>*,1,2,3</sup>,
                <a href="https://maxxxzdn.github.io/" target="_blank">Maurice Weiler</a><sup>*,1</sup>,
                <a href="https://maxxxzdn.github.io/" target="_blank">Ana Lucic</a><sup>4</sup>,
                <a href="https://maxxxzdn.github.io/" target="_blank">Johannes Brandstetter</a><sup>5,6</sup>,
                <a href="https://maxxxzdn.github.io/" target="_blank">Patrick Forré</a><sup>1,2</sup>
              </span>
            </div>
            <div class="is-size-7 publication-authors">
              <span class="author-block">
                  <sup>*</sup>equal contribution,
                  <sup>1</sup>AMLab, University of Amsterdam,
                  <sup>2</sup>AI4Science Lab, University of Amsterdam,
                  <sup>3</sup>Anton Pannekoek Institute for Astronomy,
                  <sup>4</sup>AI4Science, Microsoft Research,
                  <sup>5</sup>ELLIS Unit Linz,
                  <sup>6</sup>NXAI GmbH
              </span>
            </div>
            <div class="is-size-4 publication-authors">
              <br>ICML 2024
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.14730" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2402.14730.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/maxxxzdn/clifford-group-equivariant-cnns" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>
          <span class="link-block">
            <a href="https://colab.research.google.com/drive/1M196l6XWi56WJRCVJjd-VCWqzrtsboN2?usp=sharing" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-laptop-code"></i>
            </span>
            <span>Google Colab</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<section class="section hero is-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      <div class="column text-column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of \(E(p, q)\)-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces. They cover, for instance, \(E(3)\)-equivariance and Poincaré-equivariance on Minkowski spacetime. Our approach is based on an implicit parametrization of \(O(p,q)\)-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
          </p>
        </div>
      </div>
      <div class="column image-column">
        <img src="../assets/CSCNNs_/static/images/cscnns_main.png" style="padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="Title Image" class="teaser-image">
      </div>
    </div>
  </div>
</section>

<section class="section hero is-custom-light">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      <div class="column image-column">
        <div style="text-align: center; margin-bottom: 40px;">
          <img src="../assets/CSCNNs_/static/images/sl6a.png" style="border: 1px solid #000; margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">Clocks moving with different velocities in a rod.</p>
        </div>
        <div style="text-align: center; margin-bottom: 40px;">
          <img src="../assets/CSCNNs_/static/images/sl6b.png" style="border: 1px solid #000; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">Spacetime diagram: where the clocks end up showing the same time. 
            <br> Adapted from <a href="https://sites.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/spacetime/index.html">the blog post</a>.</p>
        </div>
      </div>
      
      <div class="column text-column">
        <h2 class="title is-3">Spacetime</h2>
        <div class="content has-text-justified">
          <p>
            In classical physics, space and time are treated as separate entities:
            space is described using Euclidean geometry, and time is considered a continuous one-dimensional flow.
            This separation leads to different notions of distance in space and time.
            In space, to measure how far apart two points are from each other, we use the Euclidean distance:
            \[
            d^2 = \Delta x^2 + \Delta y^2 + \Delta z^2
            \]
            Time, on the other hand, is thought to be absolute: 
            if we were to place two clocks at different locations,
            they would read the same time, regardless of their spatial separation or relative motion.
            In other words, the timing of events is assumed to be the same for all observers.
          </p>
          <p>
            In relativistic physics, the situation is different. 
            There, space and time are unified into a single entity - spacetime. 
            It is described by Minkowski geometry, where the distance between two points is given by:
            \[
            d^2 = c^2 \Delta t^2 - (\Delta x^2 + \Delta y^2 + \Delta z^2)
            \]
            where \(c\) is the speed of light. 

            To better build an intuition, let us imagine four clocks moving at different velocities along a rod,
            and velocities are close to the speed of light.
            As time is compressed in the direction of motion at high velocities, 
            the clocks have to travel longer distances to show the same time.
            If we now draw the trajectories of the clocks in spacetime, we would see that they end up on the same hyperbola.
            This hyperbola represents the set of all spacetime points that are separated from the origin by a fixed proper time interval.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-custom-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Pseudo-Euclidean spaces</h2>
        <div class="content has-text-justified">
          <p>
          In the previous example, we saw that events at the same spacetime distance from an origin form a hyperbola in spacetime. 
          This contrasts with Euclidean geometry, where points at the same distance from an origin form a circle in 2D or a sphere in 3D.
          In the right figure, you can visually compare the two geometries, 
          with different colors representing loci of points at the same distance from the origin.
          </p>
          <p>
          Minkowski spacetime differs from Euclidean space in that distances can be negative.
          Mathematically, it falls under the umbrella of pseudo-Euclidean spaces - 
          a generalization of Euclidean spaces to include negative distances. 
          Pseudo-Euclidean spaces are denoted as \(\mathbb{R}^{p,q}\),
          where \(p\) is the number of time-like dimensions and \(q\) is the number of space-like dimensions,
          and the tuple \((p,q)\) is called the signature of the space. 
          Note that when \(p=0\) and \(q=3\), we obtain the 3D Euclidean space, and when \(p=1\) and \(q=3\), we obtain the 4D Minkowski spacetime.
          </p>
        </div>
      </div>

      <div class="column image-column">
        <div style="text-align: center; margin-top: 40px; margin-bottom: 40px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/o_pq_orbits.png" style="margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Euclidean space (left) and Minkowski spacetime (right). 
            Colors depict different loci of points at the same distance from the origin.</p>
        </div>
      </div>

    </div>
  </div>
</section>


<section class="section hero is-custom-light">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/opq_transform.png" style="border: 1px solid #000; margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Isometry transformations of the Euclidean space (top) and Minkowski spacetime (bottom). 
            Colors depict different loci of points at the same distance from the origin.</p>
        </div>
      </div>
      
      <div class="column text-column">
        <h2 class="title is-3">Isometries</h2>
        <div class="content has-text-justified">
          <p>
          We can now look at the transformations that preserve the distance in pseudo-Euclidean spaces - isometries.
          For Euclidean spaces, these are rotations, translations, and reflections which constitute the Euclidean group \(E(n)\).
          For pseudo-Euclidean spaces, the set of isometries includes translations, spatial rotations, 
          reflections, and also boosts between inertial frames.
          They constitute the pseudo-Euclidean group \(E(p,q)\), 
          which as special cases includes the Poincaré group for Minkowski spacetime \(E(1,3)\) 
          and the Euclidean group \(E(0,n)\).
          </p>
          <p>
          Mathematically, the pseudo-Euclidean group is defined as a semidirect product of the pseudo-orthogonal group \(O(p,q)\) 
          (origin-preserving transformations) 
          and the translation group \( (\mathbb{R}^{p+q}, +) \):
          \[
          E(p,q) := O(p,q) \ltimes (\mathbb{R}^{p+q}, +).
          \]
          This means that any isometry can be represented as a composition of a linear transformation 
          from the pseudo-orthogonal group (O(p,q)) and a translation.
          </p>
        </div>
      </div>

    </div>
  </div>
</section>

<!---
<section class="section hero is-custom-normal">
  <div class="container is-max-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Deep Learning</h2>
        <div class="content has-text-justified">
          <p>
            Deep learning has adapted well to the Euclidean space.
            These days, it is fairly standard practice to process dynamical data as snapshots in time, 
            treating time as a channel dimension in the input data, 
            similar to what color channels are in images. While such representation is justified for classical physics, 
            it is not suitable for data defined on spacetime. As we discussed, here, time is essentially a different dimension,
            hence it should be modelled as a part of the grid/mesh, not as a channel.
            Furthermore, we cannot simply take existing architectures, as they are designed to handle Euclidean spaces and 
            hence will not respect the structure of spacetime.
          </p>
          <p>
            What does it mean for data to be defined on spacetime or any other space?
            The key concept here is feature vector fields:
            \[
            f: \mathbb{R}^{p,q} \to W,
            \]
            where \(W\) is a vector space. Basically, it is a function that assigns a feature vector to each point in the space.
            For example, in case of RGB images, \(p = 0, q = 2\) and \(W = \mathbb{R}^3\) is the color space. 
            We now want to learn a non-linear map 
            
          </p>
        </div>
      </div>

      <div class="column image-column">
        <div style="text-align: center; margin-top: 40px; margin-bottom: 40px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/space_examples.png" style="margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Euclidean space (left) and Minkowski spacetime (right). 
            Colors depict different loci of points at the same distance from the origin.</p>
        </div>
      </div>

    </div>
  </div>
</section>
--->

<section class="section hero is-custom-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Data on geometric spaces </h2>
        <div class="content has-text-justified">
          <p>
          We are interested in working with signals on spacetime, or any other geometric space. 
          It is convenient to formalize data as functions from the space to some feature vector space \(W\):
          \[
          f: \mathbb{R}^{p,q} \to W,
          \]
          to which we refer as feature vector fields. One can think of it as a function that assigns a feature vector to each point in the space,
          for example an RGB vector to each pixel in an image.
        </p>
        <p>
          It is reasonable to assume that when a transformation \(g \in E(p,q)\) is applied to the input space \(\mathbb{R}^{p,q}\),
          it will also affect the feature vector field \(f\). 
          To specify how the feature vector field \(f\) transforms under group actions \(g\), 
          we equip it with a group representation \(\rho\):
          \[
          \rho(g) f(x) = f(g^{-1} x),
          \]
          where \(x \in \mathbb{R}^{p,q}\) is a point in the space. 
          This equation essentially states that applying a transformation to the space and then evaluating the function 
          is equivalent to evaluating the function first and then transforming its output.
          Common examples include scalar fields like grey-scale images and temperature distributions 
          and vector fields such as wind velocity or electromagnetic fields. 
        </p>
        </div>
      </div>

      <div class="column image-column">
        
        <div style="text-align: center; margin-top: -50px;">
          <img src="../assets/CSCNNs_/static/images/cat_space.png" style="margin-top: 10px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">Group acts on data through group representations.</p>
        </div>
        <div style="text-align: center; margin-top: 10px;">
          <img src="../assets/CSCNNs_/static/images/fields.png" style="margin-top: 10px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">Different types of feature fields: scalar and vector. 
            <br> Source: <a href="https://github.com/QUVA-Lab/escnn">escnn</a>.
            </p>
        </div>
      </div>

    </div>
  </div>
</section>


<section class="section hero is-custom-light">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/lizard.png" style="border: 1px solid #000; margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 500px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Convolution with \(G\)-steerable kernels commutes with group action. 
            <br> Source: <a href="https://maurice-weiler.gitlab.io/#cnn_book">Maurice's book</a>.
          </p>
        </div>
      </div>
      
      <div class="column text-column">
        <h2 class="title is-3">Equivariant (Steerable) CNNs</h2>
        <div class="content has-text-justified">
          <p>
            If we now want to learn a map \(F: f_{\text{in}} \to f_{\text{out}}\), we must respect the geometry of the space.
            Consequently, the map should respect the transformation laws of the feature fields, which imposes the equivariance constraint:
            \[
            F \circ \rho_\text{in}(g) = \rho_{\text{out}}(g) \circ F.
            \]
            This is nothing but a consistency requirement: 
            the function should work the same way regardless of the symmetry transformation applied to the input.
          </p>  
          <p>
            Standard CNNs are already translation equivariant by design, as the same kernel is applied at each location in the input. 
            A lot of work has been done to extend this to the full set of isometries of the Euclidean space, 
            leading to the framework of Steerable CNNs. The core idea here is to use the convolutional operator, as it guarantees 
            translation equivariance, and constrain the space of kernels to be \(O(p,q)\)-equivariant. More formally, a
            kernel \(k\) should satisfy the following property, called steerability constraint: 
            \[
            k(gx) = \rho_{\text{out}}(g) k(x) \rho_{\text{in}}(g)^{-1}.
            \]
            The procedure then would be to solve the constraint by hand, obtain the kernel basis, and simply use it in the standard CNN.
            This approach is general, although quite elaborate, as one needs to solve the constraint for every group of interest \(G\).
          </p>  
        </div>
      </div>

    </div>
  </div>
</section>
  
<section class="section hero is-custom-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Implicit steerable kernels</h2>
        <div class="content has-text-justified">
          <p>
            A viable alternative to solving the steerability constraint by hand is to solve it implicitly. 
            In a nutshell, one only needs to parameterize the kernel as a function using a \(G\)-equivariant MLP,
            and the resulting convolution is theoretically guaranteed to be \(G\)-equivariant 
            (see <a href="https://arxiv.org/abs/2212.06096"> the paper</a> for more details). The idea here is to compute the kernel matrix
            for each point of the kernel grid. In other words, we use the original definition of a kernel as a function:
            \[
            k: \mathbb{R}^{n} \to \mathbb{R}^{c_{\text{out}} \times c_{\text{in}}}
            \]
            where \(\mathbb{R}^{n}\) is the n-dimensional base space, and \(c_{\text{out}}, c_{\text{in}}\) are the number of output and input channels.
            Thus, implicit kernels take a relative position as input and output the corresponding kernel matrix in vectorized form.
            This operation is efficient as it is done in parallel for all points in the kernel grid and can be cached during inference.
        </p>
        <p>
          Implicit kernel approach is very general and spares us from solving the steerability constraint by hand. 
          This is especially valuable for pseudo-Euclidean groups, as no general solution exists. Besides, 
          implementing \(G\)-equivariant MLPs is often a lot easier and plenty of ways to do so exist.
          Ideally, for our specific case of \(O(p,q)\)-equivariant CNNs, we would like to have 
          a way to build an \(O(p,q)\)-equivariant MLP for any \(p,q\) within a single framework.
          This is the reason why we employ Clifford algebra based neural networks, as they generalize to any \(p,q\).
        </p>
        </div>
      </div>

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 40px; margin-left: 20px;">
          <img src="../assets/img/publication_preview/imp_kernels.png" style="margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">
            \(G\)-equivariant MLP computes the kernel matrix for each point of the space. 
            <br> Source: <a href="https://arxiv.org/abs/2212.06096">Implicit Steerable kernels paper</a>.
          </p>
        </div>
      </div>

    </div>
  </div>
</section>  

<section class="section hero is-custom-light">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/rho_cl.png" style="border: 1px solid #000; margin-top: 80px; padding: 10px 20px 10px 20px; background-color: white; width: 500px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> CGENNs operate on multivectors in an \(O(p,q)\)-equivariant way. 
            <br> Source: <a href="https://arxiv.org/abs/2305.11141">Clifford Group Equivariant NNs</a>.
          </p>
        </div>
      </div>
      
      <div class="column text-column">
        <h2 class="title is-3">Clifford Group Equivariant NNs</h2>
        <div class="content has-text-justified">
          <p>
          We employ Clifford-algebra based neural networks as they allow very easily to build \(O(p,q)\)-equivariant MLPs 
          (see <a href="https://arxiv.org/abs/2305.11141"> the CEGNN paper</a> for more details).
          The main advantage is that the framework trivially generalizes to pseudo-orthogonal groups \(O(p,q)\) regardless
          of the dimension or metric signature of the space. Furthermore, it is possible to have a single implementation 
          of an \(O(p,q)\)-equivariant MLP, which ultimately allows us to build \(E(p,q)\)-equivariant CNNs within a single framework.
        </p>
        <p>
          The core idea of CEGNNs is to build the nonlinear map by first applying a linear transformation of each grade of the input multivector,
          and taking the geometric product with the original input. As geometric product is equivariant, the resulting map is \(O(p,q)\)-equivariant. 
          Nonlinearities are efficiently implemented through a gating mechanism: 
          a standard non-linearity (e.g., ReLU) is applied to the scalar part of the multivector, and the result is then multiplied with the original multivector.
        </p>
        </div>
      </div>

    </div>
  </div>
</section>

<section class="section hero is-custom-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Clifford-Steerable CNNs</h2>
        <div class="content has-text-justified">
          <p>
            Once we are equipped with \(O(p,q)\)-equivariant MLPs, we can build Clifford-Steerable CNNs. 
            The way the implicit kernels works is given below:
            <ol>
              <li>Given kernel size, discretize \([-1,1]^{p+q}\) to compute the grid as relative positions. </li>
              <li>Embed the relative positions as vector part of multivectors \(\to\) batch.</li>
              <li>Compute multivector matrices (vectorized) for each element of the batch.</li>
              <li>Partially evaluate the geometric product by multiplying the batch of matrices with Cayley table of the algebra.</li>
              <li>Reshape the output to \((C_{\text{out}} \cdot 2^{p+q},C_{\text{in}} \cdot 2^{p+q}, X_1, ..., X_{p+q})\).</li>
            </ol>
            The resulting tensor is ready to be used in any CNN implementation, e.g. torch.nn.ConvNd or jax.lax.conv:
            <ol>
              <li> Given: input multivector field of shape \((B, C_{\text{in}}, X_1, ..., X_{p+q}, 2^{p+q})\).</li> 
              <li> Compute the kernel as described above.</li>
              <li> Reshape the input field to \((B, C_{\text{in}} \cdot 2^{p+q}, X_1, ..., X_{p+q})\).</li>
              <li> Perform the convolution operation \(\to\) output multivector field of shape \((B, C_{\text{out}} \cdot 2^{p+q}, X_1, ..., X_{p+q})\).</li>
              <li> Reshape the output field to \((B, C_{\text{out}}, X_1, ..., X_{p+q}, 2^{p+q})\).</li>
            </ol>
          </p>
        <p>
        </p>
        </div>
      </div>

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 40px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/cscnn_scheme.png" style="margin-top: 40px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">
            Multivector kernels are computed at each point of the grid, and then the geometric product is partially evaluated to yield Clifford-Steerable kernels. 
          </p>
        </div>
      </div>

    </div>
  </div>
</section>  

<section class="section hero is-custom-light">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">

      <div class="column image-column">
        <div style="text-align: center; margin-top: 0px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/experiments.png" style="border: 1px solid #000; margin-top: 0px; padding: 10px 20px 10px 20px; background-color: white; width: 400px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> MSE on different forecasting tasks (one-step loss) as a function of number of training simulations. </p>
        </div>
      </div>
      
      <div class="column text-column">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
        <p>
          We evaluated our model on three forecasting tasks: Navier-Stokes on \(\mathbb{R}^{2}\), 
          Maxwell's equations on \(\mathbb{R}^{3}\), 
          and relativistic electrodynamics on \(\mathbb{R}^{1,2}\)
          (dataset available <a href="https://github.com/maxxxzdn/clifford-group-equivariant-cnns/tree/main/datasets/data/maxwell2d/datagen">here</a>).
          The primary difference between these tasks is that in the first two, time steps are treated as 
          feature channels, while in the last one, time is a separate dimension, i.e. part of the grid. 
          For the first two tasks, a single time step is predicted from 4 previous ones, while for the last one,
          16 previous time steps are mapped to 16 future ones. All models have similar number of parameters.
        </p>
        <p>
          First thing to notice is the extreme sample complexity of CS-CNNs. Compared to standard CNNs,
          they need 80x less training simulations to reach the same performance on the Navier-Stokes task.
          Besides, for the given parameter count, we did not observe non-equivariant models to catch up with CS-CNNs
          as the number of training simulations increases.
        </p>
        <p>
          Second, CS-CNNs generalize to arbitrary isometries of the base space, hence they work out of the box for 
          any orientation of the grid. Moreover, in the relativistic case, the boost-equivariance of CS-CNNs 
          results in more stable feature fields when viewed from the comoving frame.
        </p>
        </div>
      </div>

    </div>
  </div>

  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">

      <div class="column image-column">
        <div style="text-align: center; margin-top: 0px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/comparison.jpg" style="border: 1px solid #000; margin-top: 50px; padding: 10px 20px 10px 20px; background-color: white; width: 500px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Visual comparison of
            target and predicted fields. <br> <em>Left</em>: Navier-Stokes. <em>Right</em>: Relativistic electrodynamics. </p>
        </div>
      </div>
      
      <div class="column image-column">
        <div style="text-align: center; margin-top: 0px; margin-bottom: 0px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/comoving.gif" style="border: 1px solid #000; margin-top: 50px; padding: 0px 0px 0px 0px; background-color: white; width: 390px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;"> Boost-equivariance: our model produces consistent predictions that are stable when viewed from the comoving frame. 
            ResNet is not boost-equivariant and produces inconsistent predictions. </p>
          </p>
        </div>
      </div>

    </div>
  </div>
</section>

<section class="section hero is-custom-normal">
  <div class="container is-wider-desktop">
    <div class="columns is-centered content-columns">
      
      <div class="column text-column">
        <h2 class="title is-3">Miscellaneous</h2>
        <div class="content has-text-justified">
          Theoretically, there is a gap between CS-CNNs and Steerable CNNs in terms of the completeness of the kernel basis.
          Specifically, CS-CNNs do not cover the full space, and there are two main reasons for that:
          <ol>
            <li>
              <b>Multivector representations</b>: multivectors are not able to represent irreps with frequencies higher than 1.
              This can be illustrated using the (O(3)) group: 
              the vector/bivector part of a multivector is 3-dimensional, whereas irreps with angular frequency 2 are 5-dimensional.
              It follows that it is not possible neither to represent such irreps (e.g. a tensor) nor to learn a map that involves them.
              The latter is precisely the reason why the kernel basis of CS-CNNs is not complete 
              - vector to vector mapping requires frequency 2 kernels, which are not present in the basis. This issue is, however,
              alleviated when applying multiple convolutional layers, which recover the missing degrees of freedom (see Appendix B in the paper for proof).
            </li>
            <li>
              <b>Kernel's input</b>: the kernel's input is limited to the relative position and its norm (shell), 
              which form the 0- and 1-grade components of the input multivector. 
              With this input, the CEGNN backbone is theoretically not able to learn multivectors with non-zero 2-grade.
              Precisely, the reason for that is that 2-grade appears from the interaction of two unique 1-grades,
              which is not possible as only one 1-grade is present in the input. 
              There are, of course, ways to overcome this limitation,
              which we did not explore in this work, but I would be happy to chat about it.
            </li>
          </ol>
        <p>
        Overall, a lot of work can be done to improve the model, both at the high level (improving expressivity of implicit kernels) 
        and at the low level (developing more expressive basis based on multivectors). 
        Let me know if you are interested in collaboration or just want to chat about the paper :)
        </p>
        </div>
      </div>

      <div class="column image-column">
        <div style="text-align: center; margin-top: 20px; margin-bottom: 40px; margin-left: 20px;">
          <img src="../assets/CSCNNs_/static/images/miscel.png" style="margin-top: 150px; padding: 10px 20px 10px 20px; background-color: white; width: 500px;" alt="pseudo-Euclidean spaces">
          <p style="margin-top: -15px;">
            Compared to the <a href="https://arxiv.org/abs/1911.08251">Steerable CNNs</a>, the kernel basis of CS-CNNs 
            misses certain degrees of freedom.
          </p>
        </div>
      </div>

    </div>
  </div>
</section>  


<section class="section hero is-custom-light">
  <div class="container">
    <h2 class="title is-3">Poster</h2>
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <div class="poster-container" style="width: 100%; max-width: 1000px;">
          <img src="../assets/CSCNNs_/static/pdfs/icml2024_poster.png" alt="Research Poster">
        </div>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{
        Zhdanov2024CliffordSteerableCN,
        title = {Clifford-Steerable Convolutional Neural Networks},
        author = {Maksim Zhdanov and David Ruhe and Maurice Weiler and Ana Lucic and Johannes Brandstetter and Patrick Forr'e},
        booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
        year = {2024},
    }
    </code></pre>
  </div>
</section>


</body>
</html>
